{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default web browser has been opened at https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize. Please continue the login in the web browser. If no web browser is available or if the web browser fails to open, use device code flow with `az login --use-device-code`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive authentication successfully completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 14:24:17.979449 | ActivityCompleted: Activity=_dataflow, HowEnded=Failure, Duration=0.2 [ms], Info = {'activity_id': '10ee88e7-752f-4a37-aa04-8a9339e76b26', 'activity_name': '_dataflow', 'activity_type': 'InternalCall', 'app_name': 'dataset', 'source': 'azureml.dataset', 'version': '1.54.0', 'dataprepVersion': '', 'sparkVersion': '', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': '', 'completionStatus': 'Success', 'durationMs': 0.08}, Exception=ImportError; Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"/usr/local/bin/python3\" -m pip install azureml-dataset-runtime --upgrade\n",
      "Failed to fetch RSLex YAML representation for dataset=unregistered-73779a88-9150-49dd-8adb-9900df90a420 from workspace=Workspace.create(name='smartcreditml', subscription_id='40cb260c-04c0-4f19-90c8-a708b6e6b8f1', resource_group='smartcredit')=, got error: 'Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"/usr/local/bin/python3\" -m pip install azureml-dataset-runtime --upgrade'\n",
      "2023-11-13 14:24:17.982091 | ActivityCompleted: Activity=_dataflow, HowEnded=Failure, Duration=0.79 [ms], Info = {'activity_id': '44917136-d884-4d0a-801f-772e040e4d5e', 'activity_name': '_dataflow', 'activity_type': 'InternalCall', 'app_name': 'dataset', 'source': 'azureml.dataset', 'version': '1.54.0', 'dataprepVersion': '', 'sparkVersion': '', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': '', 'completionStatus': 'Failure', 'durationMs': 0.2}, Exception=ImportError; Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"/usr/local/bin/python3\" -m pip install azureml-dataset-runtime --upgrade\n",
      "2023-11-13 14:24:14.340864 | ActivityCompleted: Activity=get_by_name, HowEnded=Failure, Duration=3643.74 [ms], Info = {'activity_id': 'c94a2c3a-d00a-465d-a329-0e7672319221', 'activity_name': 'get_by_name', 'activity_type': 'PublicApi', 'app_name': 'dataset', 'source': 'azureml.dataset', 'version': '1.54.0', 'dataprepVersion': '', 'sparkVersion': '', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': ''}, Exception=ImportError; Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"/usr/local/bin/python3\" -m pip install azureml-dataset-runtime --upgrade\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"/usr/local/bin/python3\" -m pip install azureml-dataset-runtime --upgrade",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/azureml/data/_dataset_rest_helper.py:143\u001b[0m, in \u001b[0;36m_init_dataset\u001b[0;34m(workspace, dataset_type, dataflow_json, properties, registration, dataset)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     dataset\u001b[39m.\u001b[39;49m_dataflow\u001b[39m.\u001b[39m_rs_dataflow_yaml \u001b[39m=\u001b[39m _fetch_rslex_dataflow_yaml(workspace, fetch_id)\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/azureml/data/_loggerfactory.py:132\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    133\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/azureml/data/abstract_dataset.py:221\u001b[0m, in \u001b[0;36mAbstractDataset._dataflow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_registration \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_registration\u001b[39m.\u001b[39mworkspace:\n\u001b[0;32m--> 221\u001b[0m     dataprep()\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39m_datastore_helper\u001b[39m.\u001b[39m_set_auth_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_registration\u001b[39m.\u001b[39mworkspace)\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_definition, dataprep()\u001b[39m.\u001b[39mDataflow):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/azureml/data/_dataprep_helper.py:36\u001b[0m, in \u001b[0;36mdataprep\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dataprep_installed():\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(get_dataprep_missing_message())\n\u001b[1;32m     37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mazureml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataprep\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_dprep\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"/usr/local/bin/python3\" -m pip install azureml-dataset-runtime --upgrade",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/eduardoangeli/Documents/GitHub/smartcredit/model_azure.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eduardoangeli/Documents/GitHub/smartcredit/model_azure.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m workspace_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msmartcreditml\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eduardoangeli/Documents/GitHub/smartcredit/model_azure.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m workspace \u001b[39m=\u001b[39m Workspace(subscription_id, resource_group, workspace_name)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eduardoangeli/Documents/GitHub/smartcredit/model_azure.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mget_by_name(workspace, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msmartcreditDW\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoangeli/Documents/GitHub/smartcredit/model_azure.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m dataset\u001b[39m.\u001b[39mto_pandas_dataframe()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/azureml/data/_loggerfactory.py:132\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mwith\u001b[39;00m _LoggerFactory\u001b[39m.\u001b[39mtrack_activity(logger, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, activity_type, custom_dimensions) \u001b[39mas\u001b[39;00m al:\n\u001b[1;32m    131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    133\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(al, \u001b[39m'\u001b[39m\u001b[39mactivity_info\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39merror_code\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/azureml/data/abstract_dataset.py:93\u001b[0m, in \u001b[0;36mAbstractDataset.get_by_name\u001b[0;34m(workspace, name, version)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[39m@track\u001b[39m(_get_logger, activity_type\u001b[39m=\u001b[39m_PUBLIC_API)\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_by_name\u001b[39m(workspace, name, version\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatest\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     82\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get a registered Dataset from workspace by its registration name.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[39m    :param workspace: The existing AzureML workspace in which the Dataset was registered.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m    :rtype: typing.Union[azureml.data.TabularDataset, azureml.data.FileDataset]\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     dataset \u001b[39m=\u001b[39m AbstractDataset\u001b[39m.\u001b[39;49m_get_by_name(workspace, name, version)\n\u001b[1;32m     94\u001b[0m     AbstractDataset\u001b[39m.\u001b[39m_track_lineage([dataset])\n\u001b[1;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/azureml/data/abstract_dataset.py:898\u001b[0m, in \u001b[0;36mAbstractDataset._get_by_name\u001b[0;34m(workspace, name, version)\u001b[0m\n\u001b[1;32m    893\u001b[0m         _get_logger()\u001b[39m.\u001b[39mwarning(\u001b[39m'\u001b[39m\u001b[39mTried to retrieve v2 data asset but could not find v2 data asset\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    894\u001b[0m                               \u001b[39m'\u001b[39m\u001b[39mregistered with name \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m in the workspace.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    895\u001b[0m                               \u001b[39m.\u001b[39mformat(name, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m version \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlatest\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m (version: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(version)))\n\u001b[1;32m    896\u001b[0m     \u001b[39mraise\u001b[39;00m result\n\u001b[0;32m--> 898\u001b[0m dataset \u001b[39m=\u001b[39m _dto_to_dataset(workspace, result)\n\u001b[1;32m    899\u001b[0m warn_deprecated_blocks(dataset)\n\u001b[1;32m    900\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/azureml/data/_dataset_rest_helper.py:95\u001b[0m, in \u001b[0;36m_dto_to_dataset\u001b[0;34m(workspace, dto)\u001b[0m\n\u001b[1;32m     92\u001b[0m     dataset \u001b[39m=\u001b[39m FileDatasetFactory\u001b[39m.\u001b[39mfrom_files((store, data_path\u001b[39m.\u001b[39mrelative_path))\n\u001b[1;32m     93\u001b[0m     dataset\u001b[39m.\u001b[39m_registration \u001b[39m=\u001b[39m registration\n\u001b[0;32m---> 95\u001b[0m \u001b[39mreturn\u001b[39;00m _init_dataset(workspace\u001b[39m=\u001b[39;49mworkspace,\n\u001b[1;32m     96\u001b[0m                      dataset_type\u001b[39m=\u001b[39;49mdto\u001b[39m.\u001b[39;49mdataset_type,\n\u001b[1;32m     97\u001b[0m                      dataflow_json\u001b[39m=\u001b[39;49mdataflow_json,\n\u001b[1;32m     98\u001b[0m                      properties\u001b[39m=\u001b[39;49mdto\u001b[39m.\u001b[39;49mlatest\u001b[39m.\u001b[39;49mproperties,\n\u001b[1;32m     99\u001b[0m                      registration\u001b[39m=\u001b[39;49mregistration,\n\u001b[1;32m    100\u001b[0m                      dataset\u001b[39m=\u001b[39;49mdataset)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/azureml/data/_dataset_rest_helper.py:147\u001b[0m, in \u001b[0;36m_init_dataset\u001b[0;34m(workspace, dataset_type, dataflow_json, properties, registration, dataset)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    145\u001b[0m     _get_logger()\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFailed to fetch RSLex YAML representation for dataset=\u001b[39m\u001b[39m{\u001b[39;00mfetch_id\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    146\u001b[0m                           \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfrom workspace=\u001b[39m\u001b[39m{\u001b[39;00mworkspace\u001b[39m}\u001b[39;00m\u001b[39m=, got error: \u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m     dataset\u001b[39m.\u001b[39;49m_dataflow\u001b[39m.\u001b[39m_rs_dataflow_yaml \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# still want an attribute even if not fetched successfully\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/azureml/data/_loggerfactory.py:132\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mwith\u001b[39;00m _LoggerFactory\u001b[39m.\u001b[39mtrack_activity(logger, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, activity_type, custom_dimensions) \u001b[39mas\u001b[39;00m al:\n\u001b[1;32m    131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    133\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(al, \u001b[39m'\u001b[39m\u001b[39mactivity_info\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39merror_code\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/azureml/data/abstract_dataset.py:221\u001b[0m, in \u001b[0;36mAbstractDataset._dataflow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[39mraise\u001b[39;00m UserErrorException(\u001b[39m'\u001b[39m\u001b[39mDataset definition is missing. Please check how the dataset is created.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_registration \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_registration\u001b[39m.\u001b[39mworkspace:\n\u001b[0;32m--> 221\u001b[0m     dataprep()\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39m_datastore_helper\u001b[39m.\u001b[39m_set_auth_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_registration\u001b[39m.\u001b[39mworkspace)\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_definition, dataprep()\u001b[39m.\u001b[39mDataflow):\n\u001b[1;32m    223\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/azureml/data/_dataprep_helper.py:36\u001b[0m, in \u001b[0;36mdataprep\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdataprep\u001b[39m():\n\u001b[1;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dataprep_installed():\n\u001b[0;32m---> 36\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(get_dataprep_missing_message())\n\u001b[1;32m     37\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mazureml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataprep\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_dprep\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     check_min_version()\n",
      "\u001b[0;31mImportError\u001b[0m: Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"/usr/local/bin/python3\" -m pip install azureml-dataset-runtime --upgrade"
     ]
    }
   ],
   "source": [
    "\n",
    "# azureml-core of version 1.0.72 or higher is required\n",
    "# azureml-dataprep[pandas] of version 1.1.34 or higher is required\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "subscription_id = '40cb260c-04c0-4f19-90c8-a708b6e6b8f1'\n",
    "resource_group = 'smartcredit'\n",
    "workspace_name = 'smartcreditml'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "merged_data = Dataset.get_by_name(workspace, name='smartcreditDW')\n",
    "merged_data.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical columns ( encoding to categorical variables)\n",
    "encoded_data = pd.get_dummies(merged_data, columns=['home_ownership', 'emp_length'])\n",
    "\n",
    "# Splitting data and scaling features (split our data into training and test sets and scale, to normalizing data)\n",
    "X = encoded_data.drop(columns=['label']) \n",
    "y = merged_data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Handle NaN values\n",
    "if np.isnan(X_train_scaled).any():\n",
    "    nan_columns = np.where(np.isnan(X_train_scaled).any(axis=0))[0]\n",
    "    for col in nan_columns:\n",
    "        col_mean = np.nanmean(X_train_scaled[:, col])\n",
    "        X_train_scaled[np.isnan(X_train_scaled[:, col]), col] = col_mean\n",
    "        X_test_scaled[np.isnan(X_test_scaled[:, col]), col] = col_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
